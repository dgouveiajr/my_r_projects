---
title: NLP in 2023 Macro Outlook Reports
author: Diogo Gouveia Jr
output:
  pdf_document: default
  html_document: default
---

Every beginning of a year, most major financial players project their macroeconomic forecasts for the current year. Since this can be a very long read, we will use machine learning techniques to try to better understand these projections

For this study we will use the following reports:

- Goldman Sachs https://lnkd.in/eKzF_2K4
- J.P. Morgan https://lnkd.in/eHb6-622
- Morgan Stanley https://lnkd.in/e2nAMjmM
- BlackRock https://lnkd.in/eYxCBRGj
- HSBC https://lnkd.in/eNfBiJvH
- NatWest https://lnkd.in/euftbUw6
- Citi https://lnkd.in/eXwA-Y4X
- Credit Suisse https://lnkd.in/e4CEK5NZ
- BNP Paribas https://lnkd.in/ec4hWEdm
- Deutsche Bank https://lnkd.in/eAWCSV_7
- Apollo Global Management, Inc. https://lnkd.in/ewwq_62M
- Wells Fargo https://lnkd.in/euMkQnKE
- BNY Mellon https://lnkd.in/ezMfVgND
- Fidelity International https://lnkd.in/eJwK6tVx


```{r}

# Reading the PDF and storing on a list

require(pdftools)
require(tm)

files <- list.files(path = "~/Documents/Investments/r_apps/my_r_projects/Reports", pattern = "pdf$")

paths <- list()

for(i in files) {
  paths <- append(paths, file.path("~/Documents/Investments/r_apps/my_r_projects/Reports", i))
}

opinions <- lapply(paths, pdf_text)

```

```{r}
# Creating a Data Base of PDF in Corpus (framework for text cleaning/analysis)

pdfdatabase <- Corpus(URISource(paths), readerControl = list(reader = readPDF))
opinions.tdm <- TermDocumentMatrix(pdfdatabase, control = list(removePunctuation = TRUE,
                                                               stopwords = TRUE,
                                                               tolower = TRUE,
                                                               stemming = FALSE,
                                                               removeNumbers = TRUE,
                                                               bounds = list(global = c(3, Inf))))


```

```{r}
#Examine 10 words at a time in across documents
inspect(opinions.tdm[1:10,])

```
```{r}
ft <- findFreqTerms(opinions.tdm, lowfreq = 50, highfreq = Inf)
as.matrix(opinions.tdm[ft,])
```
```{r}
#Sum the count of all frequently occurring words
ft.tdm <- as.matrix(opinions.tdm[ft, ])
most_occ_words <- sort(apply(ft.tdm, 1, sum), decreasing = TRUE)
```

```{r}
library(tidyverse)

data <- as.matrix(most_occ_words)

```


```{r}
data 
  

```